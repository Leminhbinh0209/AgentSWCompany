"""LLM interface and implementations"""
from abc import ABC, abstractmethod
from typing import List, Optional
import asyncio
import aiohttp


class BaseLLM(ABC):
    """Base LLM interface"""
    
    @abstractmethod
    async def aask(self, prompt: str, system_msgs: Optional[List[str]] = None) -> str:
        """Async ask LLM"""
        pass


class MockLLM(BaseLLM):
    """Mock LLM for testing without API keys"""
    
    async def aask(self, prompt: str, system_msgs: Optional[List[str]] = None) -> str:
        # Simulate async delay
        await asyncio.sleep(0.1)
        
        # Return mock response based on prompt content
        prompt_lower = prompt.lower()
        
        # Check system message first to determine the role
        if system_msgs:
            sys_msg_lower = " ".join(system_msgs).lower()
            if "product manager" in sys_msg_lower or ("prd" in sys_msg_lower and "write" in sys_msg_lower):
                return """
# Product Requirement Document

## Product Overview
A simple application based on the requirement.

## User Stories
1. As a user, I want to use the application easily
2. As a user, I need the application to be reliable

## Functional Requirements
- Core functionality as specified
- User-friendly interface
- Error handling

## Non-functional Requirements
- Performance: Fast response times
- Reliability: 99% uptime
- Security: Data protection

## Success Metrics
- User satisfaction score > 4.5/5
- Performance targets met
- Zero critical bugs
"""
            elif "architect" in sys_msg_lower or ("design" in sys_msg_lower and "system" in sys_msg_lower):
                return """
# System Design Document

## Architecture Overview
The system follows a modular architecture with clear separation of concerns.

## System Architecture
- Frontend Layer: User interface components
- Business Logic Layer: Core application logic
- Data Layer: Data persistence and storage

## Components
1. Main Application Module
   - Entry point and orchestration
   - Error handling
   - Configuration management

2. Core Functionality Module
   - Business logic implementation
   - Data processing
   - Validation

3. Utility Module
   - Helper functions
   - Common utilities
   - Shared resources

## API Design
- RESTful API endpoints
- JSON request/response format
- Standard HTTP status codes
- Error handling and validation

## Technology Stack
- Programming Language: Python
- Framework: Standard library
- Testing: Unit tests
"""
            elif "engineer" in sys_msg_lower or ("code" in sys_msg_lower and "write" in sys_msg_lower):
                return """
# Main Application Code

def main():
    \"\"\"Main application entry point\"\"\"
    print("Application starting...")
    try:
        # Initialize application
        app = Application()
        app.run()
        return True
    except Exception as e:
        print(f"Error: {e}")
        return False

class Application:
    \"\"\"Main application class\"\"\"
    
    def __init__(self):
        self.initialized = False
    
    def run(self):
        \"\"\"Run the application\"\"\"
        self.initialized = True
        print("Application is running...")
        # Main application logic here

if __name__ == "__main__":
    main()
"""
        
        # Fallback to content-based detection
        if "requirement" in prompt_lower and "write" in prompt_lower and "prd" in prompt_lower:
            return """
# Main Application
def main():
    \"\"\"Main application entry point\"\"\"
    print("Hello, World!")
    print("Application is running...")
    return True

if __name__ == "__main__":
    main()
"""
        elif "design" in prompt_lower or "architecture" in prompt_lower:
            return """
# System Design

## Architecture
- Frontend: User interface
- Backend: Business logic
- Database: Data storage

## Components
1. Main module
2. Utility functions
3. Error handling

## API Design
- RESTful endpoints
- JSON responses
- Error codes
"""
        else:
            return f"Response to: {prompt[:100]}...\n\nThis is a mock response. In production, this would be generated by an LLM."


class OpenAILLM(BaseLLM):
    """OpenAI LLM implementation"""
    
    def __init__(self, model: str = "gpt-3.5-turbo", api_key: str = None):
        self.model = model
        try:
            import openai
            self.client = openai.AsyncOpenAI(api_key=api_key)
        except ImportError:
            raise ImportError("openai package is required. Install with: pip install openai")
    
    async def aask(self, prompt: str, system_msgs: Optional[List[str]] = None) -> str:
        messages = []
        
        if system_msgs:
            for sys_msg in system_msgs:
                messages.append({"role": "system", "content": sys_msg})
        
        messages.append({"role": "user", "content": prompt})
        
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=messages
        )
        
        return response.choices[0].message.content


class VLLM(BaseLLM):
    """vLLM server implementation for localhost inference"""
    
    def __init__(
        self,
        base_url: str = "http://localhost:8000/v1",
        model: str = None,
        temperature: float = 0.2,
        max_tokens: int = 512,
        api_key: str = None
    ):
        """
        Initialize the vLLM client.
        
        Args:
            base_url: Base URL of the vLLM server (default: http://localhost:8000/v1)
            model: Model name (optional, server may have a default)
            temperature: Sampling temperature
            max_tokens: Maximum tokens to generate
            api_key: Optional API key for authentication
        """
        self.base_url = base_url.rstrip('/')
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.api_key = api_key
        
        # Ensure the URL has /v1 prefix for OpenAI-compatible API
        if not self.base_url.endswith('/v1'):
            if self.base_url.endswith('/v1/'):
                self.base_url = self.base_url.rstrip('/')
            else:
                self.base_url = f"{self.base_url}/v1"
    
    async def aask(self, prompt: str, system_msgs: Optional[List[str]] = None) -> str:
        """
        Generate text from a prompt using vLLM server.
        
        Args:
            prompt: The input text prompt
            system_msgs: Optional list of system messages
            
        Returns:
            Generated text as a string
        """
        messages = []
        
        if system_msgs:
            for sys_msg in system_msgs:
                messages.append({"role": "system", "content": sys_msg})
        
        messages.append({"role": "user", "content": prompt})
        
        payload = {
            "messages": messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        
        if self.model:
            payload["model"] = self.model
        
        headers = {"Content-Type": "application/json"}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/chat/completions",
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=300)
                ) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        raise RuntimeError(
                            f"vLLM server error (status {response.status}): {error_text}"
                        )
                    
                    result = await response.json()
                    return result["choices"][0]["message"]["content"]
        except aiohttp.ClientError as e:
            raise RuntimeError(
                f"Failed to connect to vLLM server at {self.base_url}. " +
                f"Make sure the server is running. Error: {e}"
            )

class LocalLLM(BaseLLM):
    """
    A minimal wrapper for local LLM inference using llama.cpp.
    
    Supports: Llama 2, Llama 3, Qwen 2/2.5, IBM Granite 3.0
    """
    
    def __init__(
        self,
        model_path: str="./HF_MODELS/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct.Q3_K_M.gguf",
        temperature: float = 0.2,
        max_tokens: int = 512,
        n_ctx: int = 8192
    ):
        """
        Initialize the local LLM.
        
        Args:
            model_path: Path to the GGUF model file
            temperature: Sampling temperature (0.0 = deterministic, 1.0 = creative)
            max_tokens: Maximum tokens to generate per response
            n_ctx: Context window size
        """
        try:
            from llama_cpp import Llama
        except ImportError:
            raise ImportError(
                "llama-cpp-python package is required. Install with: " +
                "pip install llama-cpp-python"
            )
        
        self.llm = Llama(
            model_path=model_path,
            temperature=temperature,
            n_ctx=n_ctx,
            verbose=False,
        )
        self.max_tokens = max_tokens
        
        # Detect model type and set appropriate template
        model_path_lower = model_path.lower()
        if "llama-2" in model_path_lower or "llama2" in model_path_lower:
            self.prompt_template = self._llama2_prompt
            self.stop_tokens = ["</s>"]
        elif "llama-3" in model_path_lower or "llama3" in model_path_lower:
            self.prompt_template = self._llama3_prompt
            self.stop_tokens = ["<|eot_id|>", "<|end_of_text|>"]
        elif "qwen2.5" in model_path_lower or "qwen-2.5" in model_path_lower:
            self.prompt_template = self._qwen2_prompt
            self.stop_tokens = ["<|im_end|>", "<|endoftext|>"]
        elif "qwen2" in model_path_lower or "qwen-2" in model_path_lower:
            self.prompt_template = self._qwen2_prompt
            self.stop_tokens = ["<|im_end|>", "<|endoftext|>"]
        elif "granite" in model_path_lower:
            self.prompt_template = self._granite_prompt
            self.stop_tokens = ["<|end_of_text|>", "<|endoftext|>"]
        else:
            raise ValueError(f"Unsupported model: {model_path}. Supported: Llama 2/3, Qwen 2/2.5, Granite 3.0")

    def _llama2_prompt(self, prompt: str, system_msgs: Optional[List[str]] = None) -> str:
        """
        Llama 2 prompt format:
        <s>[INST] <<SYS>>
        {system_message}
        <</SYS>>
        
        {user_message} [/INST]
        """
        full_prompt = "<s>[INST] "
        
        # Add system message if provided
        if system_msgs:
            system_text = "\n".join(system_msgs)
            full_prompt += f"<<SYS>>\n{system_text}\n<</SYS>>\n\n"
        
        # Add user prompt
        full_prompt += f"{prompt} [/INST]"
        
        return full_prompt

    def _llama3_prompt(self, prompt: str, system_msgs: Optional[List[str]] = None) -> str:
        """
        Llama 3 prompt format:
        <|begin_of_text|><|start_header_id|>system<|end_header_id|>
        
        {system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>
        
        {user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
        """
        full_prompt = "<|begin_of_text|>"
        
        # Add system message if provided
        if system_msgs:
            system_text = "\n".join(system_msgs)
            full_prompt += f"<|start_header_id|>system<|end_header_id|>\n\n{system_text}<|eot_id|>"
        
        # Add user prompt
        full_prompt += f"<|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|>"
        
        # Add assistant header
        full_prompt += "<|start_header_id|>assistant<|end_header_id|>\n\n"
        
        return full_prompt

    def _qwen2_prompt(self, prompt: str, system_msgs: Optional[List[str]] = None) -> str:
        """
        Qwen 2/2.5 prompt format (ChatML):
        <|im_start|>system
        {system_message}<|im_end|>
        <|im_start|>user
        {user_message}<|im_end|>
        <|im_start|>assistant
        """
        full_prompt = ""
        
        # Add system message if provided
        if system_msgs:
            system_text = "\n".join(system_msgs)
            full_prompt += f"<|im_start|>system\n{system_text}<|im_end|>\n"
        
        # Add user prompt
        full_prompt += f"<|im_start|>user\n{prompt}<|im_end|>\n"
        
        # Add assistant header
        full_prompt += "<|im_start|>assistant\n"
        
        return full_prompt

    def _granite_prompt(self, prompt: str, system_msgs: Optional[List[str]] = None) -> str:
        """
        IBM Granite 3.0 prompt format:
        <|start_of_role|>system<|end_of_role|>{system_message}<|end_of_text|>
        <|start_of_role|>user<|end_of_role|>{user_message}<|end_of_text|>
        <|start_of_role|>assistant<|end_of_role|>
        """
        full_prompt = ""
        
        # Add system message if provided
        if system_msgs:
            system_text = "\n".join(system_msgs)
            full_prompt += f"<|start_of_role|>system<|end_of_role|>{system_text}<|end_of_text|>\n"
        
        # Add user prompt
        full_prompt += f"<|start_of_role|>user<|end_of_role|>{prompt}<|end_of_text|>\n"
        
        # Add assistant header
        full_prompt += "<|start_of_role|>assistant<|end_of_role|>"
        
        return full_prompt

    async def aask(self, prompt: str, system_msgs: Optional[List[str]] = None) -> str:
        """
        Generate text from a prompt.
        
        Args:
            prompt: The input text prompt
            system_msgs: Optional list of system messages
            
        Returns:
            Generated text as a string
        """
        # Format prompt using detected template
        # full_prompt = self.prompt_template(prompt, system_msgs) # OLD
        messages = []
        
        # Add system messages if provided
        if system_msgs:
            system_text = "\n".join(system_msgs)
            messages.append({
                "role": "system",
                "content": system_text
            })
        
        # Add user prompt
        messages.append({
            "role": "user",
            "content": prompt
        })
        # Run in executor to avoid blocking
        loop = asyncio.get_event_loop()
        # response = await loop.run_in_executor(
        #     None,
        #     lambda: self.llm(
        #         prompt=full_prompt,
        #         max_tokens=self.max_tokens,
        #         stop=self.stop_tokens,
        #     )
        # )# OLD
        response = await loop.run_in_executor(
            None,
            lambda: self.llm.create_chat_completion(
                messages=messages,
                max_tokens=self.max_tokens,
                stop=self.stop_tokens,
            )
        )
        # Extract text and clean up
        # res = response["choices"][0]["text"].strip() # OLD
        res = response["choices"][0]["message"]["content"].strip() # NEW
        
        return res

def get_llm(
    local_model_path: str = "./HF_MODELS/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct.Q3_K_M.gguf",
    vllm_base_url: str = "http://localhost:8000/v1",
    vllm_model: str = None
) -> BaseLLM:
    """
    Get the best available LLM with priority:
    1. LocalLLM (llama.cpp) - if model file exists
    2. VLLM - if server is running
    3. MockLLM - fallback for testing
    
    Args:
        local_model_path: Path to GGUF model file for LocalLLM
        vllm_base_url: Base URL for vLLM server
        vllm_model: Model name for vLLM (optional)
        
    Returns:
        BaseLLM instance (LocalLLM, VLLM, or MockLLM)
    """
    import os
    
    # Priority 1: Try LocalLLM
    if os.path.exists(local_model_path):
        try:
            print(f"Initializing LocalLLM with model path:\n {local_model_path}")
            
            return LocalLLM(model_path=local_model_path)
        except Exception as e:
            print(f"Warning: Could not initialize LocalLLM: {e}")
            print("Falling back to VLLM...")
    
    # Priority 2: Try VLLM
    try:
        import aiohttp
        # Try to create VLLM instance (will fail if server not running)
        vllm = VLLM(base_url=vllm_base_url, model=vllm_model)
        # Test connection with a simple request (in a non-blocking way)
        # We'll just return it and let it fail at first use if server is down
        return vllm
    except Exception as e:
        print(f"Warning: Could not initialize VLLM: {e}")
        print("Falling back to MockLLM...")
    
    # Priority 3: Fallback to MockLLM
    print("Using MockLLM (no local model or vLLM server available)")
    return MockLLM()

