"""LLM interface and implementations"""
from abc import ABC, abstractmethod
from typing import List, Optional
import asyncio
import aiohttp


class BaseLLM(ABC):
    """Base LLM interface"""
    
    @abstractmethod
    async def aask(self, prompt: str, system_msgs: Optional[List[str]] = None) -> str:
        """Async ask LLM"""
        pass


class MockLLM(BaseLLM):
    """Mock LLM for testing without API keys"""
    
    async def aask(self, prompt: str, system_msgs: Optional[List[str]] = None) -> str:
        # Simulate async delay
        await asyncio.sleep(0.1)
        
        # Return mock response based on prompt content
        prompt_lower = prompt.lower()
        
        # Check system message first to determine the role
        if system_msgs:
            sys_msg_lower = " ".join(system_msgs).lower()
            if "product manager" in sys_msg_lower or ("prd" in sys_msg_lower and "write" in sys_msg_lower):
                return """
# Product Requirement Document

## Product Overview
A simple application based on the requirement.

## User Stories
1. As a user, I want to use the application easily
2. As a user, I need the application to be reliable

## Functional Requirements
- Core functionality as specified
- User-friendly interface
- Error handling

## Non-functional Requirements
- Performance: Fast response times
- Reliability: 99% uptime
- Security: Data protection

## Success Metrics
- User satisfaction score > 4.5/5
- Performance targets met
- Zero critical bugs
"""
            elif "architect" in sys_msg_lower or ("design" in sys_msg_lower and "system" in sys_msg_lower):
                return """
# System Design Document

## Architecture Overview
The system follows a modular architecture with clear separation of concerns.

## System Architecture
- Frontend Layer: User interface components
- Business Logic Layer: Core application logic
- Data Layer: Data persistence and storage

## Components
1. Main Application Module
   - Entry point and orchestration
   - Error handling
   - Configuration management

2. Core Functionality Module
   - Business logic implementation
   - Data processing
   - Validation

3. Utility Module
   - Helper functions
   - Common utilities
   - Shared resources

## API Design
- RESTful API endpoints
- JSON request/response format
- Standard HTTP status codes
- Error handling and validation

## Technology Stack
- Programming Language: Python
- Framework: Standard library
- Testing: Unit tests
"""
            elif "engineer" in sys_msg_lower or ("code" in sys_msg_lower and "write" in sys_msg_lower):
                return """
# Main Application Code

def main():
    \"\"\"Main application entry point\"\"\"
    print("Application starting...")
    try:
        # Initialize application
        app = Application()
        app.run()
        return True
    except Exception as e:
        print(f"Error: {e}")
        return False

class Application:
    \"\"\"Main application class\"\"\"
    
    def __init__(self):
        self.initialized = False
    
    def run(self):
        \"\"\"Run the application\"\"\"
        self.initialized = True
        print("Application is running...")
        # Main application logic here

if __name__ == "__main__":
    main()
"""
        
        # Fallback to content-based detection
        if "requirement" in prompt_lower and "write" in prompt_lower and "prd" in prompt_lower:
            return """
# Main Application
def main():
    \"\"\"Main application entry point\"\"\"
    print("Hello, World!")
    print("Application is running...")
    return True

if __name__ == "__main__":
    main()
"""
        elif "design" in prompt_lower or "architecture" in prompt_lower:
            return """
# System Design

## Architecture
- Frontend: User interface
- Backend: Business logic
- Database: Data storage

## Components
1. Main module
2. Utility functions
3. Error handling

## API Design
- RESTful endpoints
- JSON responses
- Error codes
"""
        else:
            return f"Response to: {prompt[:100]}...\n\nThis is a mock response. In production, this would be generated by an LLM."


class OpenAILLM(BaseLLM):
    """OpenAI LLM implementation"""
    
    def __init__(self, model: str = "gpt-3.5-turbo", api_key: str = None):
        self.model = model
        try:
            import openai
            self.client = openai.AsyncOpenAI(api_key=api_key)
        except ImportError:
            raise ImportError("openai package is required. Install with: pip install openai")
    
    async def aask(self, prompt: str, system_msgs: Optional[List[str]] = None) -> str:
        messages = []
        
        if system_msgs:
            for sys_msg in system_msgs:
                messages.append({"role": "system", "content": sys_msg})
        
        messages.append({"role": "user", "content": prompt})
        
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=messages
        )
        
        return response.choices[0].message.content


class VLLM(BaseLLM):
    """vLLM server implementation for localhost inference"""
    
    def __init__(
        self,
        base_url: str = "http://localhost:8000/v1",
        model: str = None,
        temperature: float = 0.2,
        max_tokens: int = 512,
        api_key: str = None
    ):
        """
        Initialize the vLLM client.
        
        Args:
            base_url: Base URL of the vLLM server (default: http://localhost:8000/v1)
            model: Model name (optional, server may have a default)
            temperature: Sampling temperature
            max_tokens: Maximum tokens to generate
            api_key: Optional API key for authentication
        """
        self.base_url = base_url.rstrip('/')
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.api_key = api_key
        
        # Ensure the URL has /v1 prefix for OpenAI-compatible API
        if not self.base_url.endswith('/v1'):
            if self.base_url.endswith('/v1/'):
                self.base_url = self.base_url.rstrip('/')
            else:
                self.base_url = f"{self.base_url}/v1"
    
    async def aask(self, prompt: str, system_msgs: Optional[List[str]] = None) -> str:
        """
        Generate text from a prompt using vLLM server.
        
        Args:
            prompt: The input text prompt
            system_msgs: Optional list of system messages
            
        Returns:
            Generated text as a string
        """
        messages = []
        
        if system_msgs:
            for sys_msg in system_msgs:
                messages.append({"role": "system", "content": sys_msg})
        
        messages.append({"role": "user", "content": prompt})
        
        payload = {
            "messages": messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        
        if self.model:
            payload["model"] = self.model
        
        headers = {"Content-Type": "application/json"}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/chat/completions",
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=300)
                ) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        raise RuntimeError(
                            f"vLLM server error (status {response.status}): {error_text}"
                        )
                    
                    result = await response.json()
                    return result["choices"][0]["message"]["content"]
        except aiohttp.ClientError as e:
            raise RuntimeError(
                f"Failed to connect to vLLM server at {self.base_url}. "
                f"Make sure the server is running. Error: {e}"
            )


class LocalLLM(BaseLLM):
    """
    A minimal wrapper for local LLM inference using llama.cpp.
    
    This class is intentionally simple and grows throughout the lessons.
    """
    
    def __init__(
        self,
        model_path: str,
        temperature: float = 0.2,
        max_tokens: int = 512,
        n_ctx: int = 2048
    ):
        """
        Initialize the local LLM.
        
        Args:
            model_path: Path to the GGUF model file
            temperature: Sampling temperature (0.0 = deterministic, 1.0 = creative)
            max_tokens: Maximum tokens to generate per response
            n_ctx: Context window size
        """
        try:
            from llama_cpp import Llama
        except ImportError:
            raise ImportError(
                "llama-cpp-python package is required. Install with: "
                "pip install llama-cpp-python"
            )
        
        self.llm = Llama(
            model_path=model_path,
            temperature=temperature,
            n_ctx=n_ctx,
            verbose=False,
        )
        self.max_tokens = max_tokens
    
    async def aask(self, prompt: str, system_msgs: Optional[List[str]] = None) -> str:
        """
        Generate text from a prompt.
        
        Args:
            prompt: The input text prompt
            system_msgs: Optional list of system messages (will be prepended to prompt)
            
        Returns:
            Generated text as a string
        """
        # Combine system messages and user prompt
        full_prompt = prompt
        if system_msgs:
            system_text = "\n".join([f"System: {msg}" for msg in system_msgs])
            full_prompt = f"{system_text}\n\nUser: {prompt}\n\nAssistant:"
        
        # Run in executor to avoid blocking
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: self.llm(
                prompt=full_prompt,
                max_tokens=self.max_tokens,
                stop=["</s>", "\n\n", "User:", "Assistant:", "System:"],
            )
        )
        
        return response["choices"][0]["text"].strip()

